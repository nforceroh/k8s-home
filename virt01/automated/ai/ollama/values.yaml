---
ollama:
  image:
    repository: harbor.nf.lab/proxy.docker.io/ollama/ollama
  ollama:
    gpu:
      enabled: true
      type: 'nvidia'
    models:
      pull:
        - mistral
        - llama3.1
        - llama3.2
        - llama3.2-vision
        - codellama
        - deepseek-coder:6.7b
        - nomic-embed-text:latest
        - qwen2.5-coder:1.5b-base

  runtimeClassName: "nvidia"
  service:
    type: LoadBalancer
    annotations:
      metallb.io/address-pool: vlan1400
      external-dns.alpha.kubernetes.io/hostname: ollama-api.k3s.nf.lab
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: nf-lab
      cert-manager.io/private-key-rotation-policy: Always
      traefik.ingress.kubernetes.io/router.entrypoints: web,websecure  
      kubernetes.io/ingress.class: traefik
      external-dns.alpha.kubernetes.io/target: traefik.k3s.nf.lab
    hosts:
    - host: ollama.k3s.nf.lab
      paths:
        - path: /
          pathType: Prefix
    tls:
    - hosts:
      - ollama.k3s.nf.lab
      secretName: ollama-tls
  serviceAccount:
    create: true
    automount: true
    name: ollama-sa
  persistentVolume:
    enabled: true
    existingClaim: ollama
    size: 50Gi

